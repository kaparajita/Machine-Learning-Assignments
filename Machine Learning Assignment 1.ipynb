{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What are the three stages to build the hypotheses or model in machine learning?\n",
    "\n",
    "a.Model building\n",
    "b.Model testing\n",
    "c.Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What is the standard approach to supervised learning?\n",
    "\n",
    "The standard approach to supervised learning is to split the set of example into the training set and the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. What is Training set and Test set?\n",
    "\n",
    "Training Set\n",
    "In machine learning, a training set is a dataset used to train a model. In training the model, specific features\n",
    "are picked out from the training set. These features are then incorporated into the model. Thereby, if the\n",
    "training set is labeled correctly, the model should be able to learn something from these features.\n",
    "\n",
    "Test Set\n",
    "The test set is a dataset used to measure how well the model performs at making predictions on that test set.\n",
    "If the prediction scores for the test set are unreasonable, we’ll need to make some adjustments to our model\n",
    "and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?\n",
    "\n",
    "Ensemble learning is a machine learning concept in which idea is to train multiple models (learners) to solve the same problem.\n",
    "\n",
    "The main advantages of Ensemble learning methods are :\n",
    "Reduced variance : Overcome overfitting problem. Low variance means model independent of training data. Results are less dependent on features of a single model and training set.\n",
    "Reduced bias : Overcome underfitting problem. Low bias means linear regression applied to linear data, second degree polynomial applied to quadratic data. \n",
    "    Combination of multiple classifiers may produce more reliable classification than single classifier.\n",
    "\n",
    "Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Generates m new training data sets.Each new training data set picks a sample of observations with replacement (bootstrap sample) from original data set.  \n",
    "By sampling with replacement, some observations may be repeated in each new training data set. The m models are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification).\n",
    "Example :\n",
    "\n",
    "Random forest is one of the most important bagging ensemble learning algorithm, In random forest, approx. 2/3rd of the total training data (63.2%) is used for growing each tree. \n",
    "And the remaining one-third of the cases (36.8%) are left out and not used in the construction of each tree. Each tree gives a classification, and we say the tree \"votes\" for that class. \n",
    "The forest chooses the classification having the most votes over all the trees in the forest. For a binary dependent variable, the vote will be YES or NO, count up the YES votes. This is the RF score and the percent YES votes received is the predicted probability. \n",
    "In regression case, it is average of dependent variable.\n",
    "\n",
    "Uses of Bagging Algorithm\n",
    "It is designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression.\n",
    "It also reduces variance and helps to avoid overfitting. \n",
    "\n",
    "Boosting Algorithm\n",
    "\n",
    "Laymans Term : AdaBoost Algorithm\n",
    "\n",
    "In this example, we focus on boosting algorithm, especially Adaboost algorithm.\n",
    "Start by creating a tree on training data, where each observation is assigned an equal weight.\n",
    "Then compute the predicted classification and weights are redetermined and assign greater weight to those observations that are difficult to classify and lower weights to those that are easy to classify.\n",
    "Weights for all observations must sum to 1.\n",
    "Second tree is grown on weighted data (weights are based on residuals or misclassification error). Idea is to improve prediction of first tree.\n",
    "Weights are redetermined and assign higher weights if it is classified incorrectly.\n",
    "New Model is now Tree 1 + Tree 2\n",
    "Compute residuals or classification error from this new 2-tree model (Tree1 + Tree2) and grow 3rd tree to predict revised residuals.\n",
    "Then subsequent trees help in classifying observations that are not well classified by preceding trees.\n",
    "The final prediction is a weighted sum of the predictions made by previous tree models. \n",
    "\n",
    "In this process, we have created many simple decision trees, where each tree is built for the prediction errors (classification error) of the previous tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. How can you avoid overfitting ?\n",
    "\n",
    "To avoid overfitting we need to consider good fitting.To understand this goal, we can look at the performance of a machine learning algorithm over time as it is\n",
    "learning a training data. We can plot both the skill on the training data and the skill on a test dataset we have\n",
    "held back from the training process.\n",
    "Over time, as the algorithm learns, the error for the model on the training data goes down and so does the\n",
    "error on the test dataset. If we train for too long, the performance on the training dataset may continue to\n",
    "decrease because the model is overfitting and learning the irrelevant detail and noise in the training dataset.\n",
    "At the same time the error for the test set starts to rise again as the model’s ability to generalize decreases.\n",
    "The sweet spot is the point just before the error on the test dataset starts to increase where the model has\n",
    "good skill on both the training dataset and the unseen test dataset.\n",
    "You can perform this experiment with your favorite machine learning algorithms. This is often not useful\n",
    "technique in practice, because by choosing the stopping point for training using the skill on the test dataset it\n",
    "means that the testset is no longer “unseen” or a standalone objective measure. Some knowledge (a lot of\n",
    "useful knowledge) about that data has leaked into the training procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
